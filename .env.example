# Move37 content-summarize configuration template
# Copy this file to `.env` and replace placeholder values.
#
# Config loading contract (from tasks.md):
# 1. Only read LLM config from `.env`.
# 2. Use `LLM_PROVIDER` to select the provider block.
# 3. Read selected provider values:
#    - LLM_<PROVIDER>_API_KEY (required)
#    - LLM_<PROVIDER>_MODEL
#    - LLM_<PROVIDER>_BASE_URL
# 4. Read common runtime params for all providers:
#    - LLM_TEMPERATURE / LLM_MAX_TOKENS / LLM_TIMEOUT / LLM_MAX_RETRIES
#
# Supported values: openai | deepseek | gemini | glm
LLM_PROVIDER=openai

# Common generation settings
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000
LLM_TIMEOUT=60
LLM_MAX_RETRIES=3

# Optional: override default prompt template.
# Must contain "{url}" placeholder.
# You can also use "{content}" placeholder.
LLM_PROMPT_TEMPLATE=

# YouTube preprocessing settings
# Subtitle language priority list, comma-separated.
YOUTUBE_TRANSCRIPT_LANGS=zh-Hans,zh,en
# Max chars sent to summarizer from fetched YouTube material.
YOUTUBE_MAX_INPUT_CHARS=20000
# If transcript content exceeds this length, chunk summarize first.
YOUTUBE_CHUNK_SIZE=4000
# Whether to fallback to metadata-only summary when transcript fetch fails.
YOUTUBE_ENABLE_METADATA_FALLBACK=true

# OpenAI (used when LLM_PROVIDER=openai)
LLM_OPENAI_API_KEY=
LLM_OPENAI_MODEL=gpt-3.5-turbo
LLM_OPENAI_BASE_URL=https://api.openai.com/v1

# DeepSeek (used when LLM_PROVIDER=deepseek)
LLM_DEEPSEEK_API_KEY=
LLM_DEEPSEEK_MODEL=deepseek-chat
LLM_DEEPSEEK_BASE_URL=https://api.deepseek.com

# Gemini (used when LLM_PROVIDER=gemini)
# Gemini SDK usually does not require base_url.
LLM_GEMINI_API_KEY=
LLM_GEMINI_MODEL=gemini-2.5-flash
LLM_GEMINI_BASE_URL=

# GLM (used when LLM_PROVIDER=glm)
LLM_GLM_API_KEY=
LLM_GLM_MODEL=glm-4
LLM_GLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4/
